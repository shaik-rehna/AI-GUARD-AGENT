{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daa74f59",
   "metadata": {},
   "source": [
    "# REPORT\n",
    "# milestone 1\n",
    "## 1. Overview\n",
    "This module implements a **voice-controlled protection system** that enables or disables a “Protect Mode” based on spoken commands.  \n",
    "It integrates **speech recognition**, **real-time webcam monitoring**, and **multithreaded execution** to achieve hands-free activation and visual feedback.\n",
    "\n",
    "When the user says **“protect my room”**, the system activates *Protect Mode*; pressing **‘q’** or issuing a stop command deactivates it.  \n",
    "All recognized speech and system actions are logged with timestamps for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. System Functionality\n",
    "\n",
    "### Voice Activation\n",
    "The system uses the `speech_recognition` library to:\n",
    "1. Continuously capture audio from the microphone.  \n",
    "2. Convert the audio to text using the **Google Speech Recognition API**.  \n",
    "3. Detect the keyword **“protect my room”** to trigger activation.  \n",
    "4. Record all speech events and outcomes in a log file (`command_log.txt`).\n",
    "\n",
    "### Webcam Monitoring\n",
    "Using **OpenCV (`cv2`)**, the webcam feed is displayed in real-time.  \n",
    "The current system status (*Protect Mode ON/OFF*) is superimposed on the video stream.  \n",
    "When the user presses **‘q’**, the video window closes and the system shuts down.\n",
    "\n",
    "### Multithreading Integration\n",
    "To enable simultaneous listening and video display:\n",
    "- The speech recognition loop runs in a **separate thread**.\n",
    "- The webcam display runs in the **main thread**.\n",
    "  \n",
    "This ensures the program remains responsive to both **voice** and **keyboard** inputs concurrently.\n",
    "\n",
    "---\n",
    "\n",
    "# Test results \n",
    "we conducted 5 tests for speech recognization and calculated the accuracy using the formula \n",
    "Accuracy=(Correct Commands/Total Commands) ​×100\n",
    "therefore our accuracy is 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f6074",
   "metadata": {},
   "source": [
    "# Face Recognition and Evaluation System\n",
    "\n",
    "## 1. Overview\n",
    "This module implements a **two-stage face recognition pipeline**:\n",
    "1. **Enrollment Phase** – captures and stores trusted user face embeddings using a webcam.  \n",
    "2. **Testing & Evaluation Phase** – compares unknown faces to enrolled users and computes recognition accuracy, precision, recall, and F1-score across different conditions.\n",
    "\n",
    "The system uses:\n",
    "- **OpenCV** (`cv2`) for real-time camera input and image handling.  \n",
    "- **face_recognition** library for encoding and matching facial features.  \n",
    "- **NumPy** for data storage and manipulation.  \n",
    "- **Scikit-learn** for performance metric evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Stage 1 – Face Enrollment\n",
    "\n",
    "### Objective\n",
    "To register trusted users by capturing their facial embeddings and saving them for future authentication.\n",
    "\n",
    "###  Implementation Details\n",
    "- The directory `trusted_faces/` stores `.npy` files for each user (e.g., `Rehna.npy`, `Yashaswini.npy`).  \n",
    "- When the user starts the script, they are prompted to **enter their name**, which determines the filename.  \n",
    "- The webcam stream is activated, and users can press:\n",
    "  - **`s`** → Capture a face and save the embedding.  \n",
    "  - **`q`** → Quit the enrollment session.\n",
    "\n",
    "#  Stage 2 – Face Recognition and Evaluation\n",
    "\n",
    "## 1. Overview\n",
    "This stage performs **face verification and performance evaluation** by comparing unknown test images against the stored trusted user embeddings.  \n",
    "It measures how accurately the system can recognize enrolled users under different environmental or visual conditions.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Objectives\n",
    "- Load all trusted user embeddings from the **enrollment phase**.  \n",
    "- Detect and encode faces in test images.  \n",
    "- Match unknown faces against the trusted database.  \n",
    "- Compute performance metrics such as **accuracy**, **precision**, **recall**, and **F1 score**.  \n",
    "- Analyze results **per condition** (lighting, camera angle, distance, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Input Structure\n",
    "\n",
    "###  Directories\n",
    "- **`trusted_faces/`** → Contains `.npy` files of stored face embeddings for each user.  \n",
    "  Example: `trusted_faces/Rehna.npy`, `trusted_faces/Yashaswini.npy`\n",
    "- **`test_cases/`** → Contains test images organized by condition folders.\n",
    "\n",
    "\n",
    "Our test results are as follows:\n",
    "\n",
    "===== OVERALL RESULTS =====\n",
    "\n",
    "Accuracy : 0.9\n",
    "\n",
    "Precision: 0.95\n",
    "\n",
    "Recall   : 0.9\n",
    "\n",
    "F1 Score : 0.913\n",
    "\n",
    "===== PER-CONDITION RESULTS =====\n",
    "\n",
    "background_noise -> Accuracy: 1.00\n",
    "\n",
    "bright_light -> Accuracy: 0.89\n",
    "\n",
    "dim_light -> Accuracy: 0.82\n",
    "\n",
    "unseen -> Accuracy: 1.00\n",
    "\n",
    "#Overall integrated code (Milestone 3,4 ) is in mile_stone_3_4.py\n",
    "#  Voice-Activated Intelligent Room Security System  \n",
    "### *(Milestone 3,4 – Windows SAPI Edition with Face Embedding Matching)*\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This project implements an **autonomous security assistant** capable of protecting a personal room using **voice, vision, and reasoning**.  \n",
    "It acts as a digital guard that activates upon the user’s command (“protect my room”) and continuously monitors the environment through the webcam.\n",
    "\n",
    "When an unknown face appears, it engages the intruder in a spoken dialogue, attempts to understand the intent through voice recognition, and decides whether to **de-escalate**, **warn**, or **trigger an alarm**.  \n",
    "All interactions, including **images** and **audio transcripts**, are recorded as **evidence** for accountability.\n",
    "\n",
    "Unlike cloud-based systems, this project is fully **offline** — running locally using:\n",
    "- **OpenCV** for video capture,  \n",
    "- **face_recognition (dlib)** for facial analysis,  \n",
    "- **SpeechRecognition** for local voice processing, and  \n",
    "- **Windows SAPI** for natural text-to-speech output.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. System Overview\n",
    "\n",
    "### Objective\n",
    "- Protect a room autonomously using **facial recognition** and **voice interaction**.  \n",
    "- Operate **hands-free**, triggered only by **voice command** or **keyboard input**.  \n",
    "- Log evidence when unrecognized persons appear.\n",
    "\n",
    "### Key Features\n",
    "-  **Voice Activation/Deactivation** – Controlled by spoken commands.  \n",
    "-  **Face Recognition** – Matches detected faces against a stored database of trusted embeddings.  \n",
    "-  **Multi-Level Escalation** – Uses dialogue-based verification of intruders.  \n",
    "-  **Evidence Recording** – Captures intruder image and conversation transcripts.  \n",
    "-  **Offline and Private** – No cloud dependencies or data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. System Architecture\n",
    "\n",
    "```plaintext\n",
    "┌────────────────────────────┐\n",
    "│     Voice / Key Activation │\n",
    "│  \"Protect my room\" / 'a'   │\n",
    "└──────────────┬─────────────┘\n",
    "               │\n",
    "               ▼\n",
    "┌────────────────────────────┐\n",
    "│    Camera Monitoring Loop  │\n",
    "│  - Frame Capture (OpenCV)  │\n",
    "│  - Face Detection          │\n",
    "│  - Face Encoding           │\n",
    "│  - Embedding Matching      │\n",
    "└──────────────┬─────────────┘\n",
    "       Known   │\n",
    "        Face   ▼\n",
    "    Continue Monitoring\n",
    "               │\n",
    "       Unknown ▼\n",
    "┌────────────────────────────┐\n",
    "│  Intruder Escalation Logic │\n",
    "│  - Dialogue Prompt(Windows SAPI)\n",
    "│  - Speech Recognition (Google Speech Recognition)     \n",
    "│  - Reply Classification    │\n",
    "└──────────────┬─────────────┘\n",
    "               │\n",
    "               ▼\n",
    "┌────────────────────────────┐\n",
    "│ Evidence Recording & Alarm │\n",
    "│  - Save Frame + Transcript │\n",
    "│  - Play Alarm (winsound)   │\n",
    "└────────────────────────────┘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d6151",
   "metadata": {},
   "source": [
    "##  Integration Challenges and Solutions\n",
    "\n",
    "### 1. Multi-Modal Synchronization (Voice, Face)\n",
    "**Challenge:**  \n",
    "Combining real-time voice activation, face recognition, and reasoning modules in a single system introduced timing and resource conflicts. The voice listener and camera feed both demanded access to system resources (microphone, CPU, GPU), leading to lag or blocking behavior.\n",
    "\n",
    "**Solution:**  \n",
    "Implemented multithreading to run voice, vision, and decision modules in parallel. Each thread manages its own I/O (OpenCV for camera, PyAudio for mic) and communicates via shared event flags. This allowed:\n",
    "- Continuous monitoring for the activation phrase (“protect my room”).\n",
    "- Parallel face recognition without interrupting voice capture.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Face Embedding Management\n",
    "**Challenge:**  \n",
    "Each enrolled user could have multiple embeddings captured under varying lighting, poses, and distances. Storing all embeddings in one large array quickly became inefficient and prone to duplication or mismatch.\n",
    "\n",
    "**Solution:**  \n",
    "A per-user `.npy` file structure was introduced (`trusted_faces/<user>.npy`), enabling incremental saving of embeddings with each capture (`s` key). This modular storage allowed:\n",
    "- Easier updates for individual users.\n",
    "- Efficient loading at runtime (`numpy.load` per user).\n",
    "- Scalable management as more trusted users were added.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Matching and Distance Threshold Calibration\n",
    "**Challenge:**  \n",
    "During recognition, the raw Euclidean distance between embeddings varied significantly depending on lighting or expression, causing both **false positives** and **false negatives**.\n",
    "\n",
    "**Solution:**  \n",
    "Through iterative testing, a threshold of **0.45** was empirically chosen as the acceptance cutoff.  \n",
    "Distances below this value were considered *same-person* matches. The calibration was guided by plotting distance distributions for known and unknown pairs, observing where the separation between the two clusters was clearest.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Evidence Logging and Escalation\n",
    "**Challenge:**  \n",
    "Ensuring that suspicious activity (unrecognized faces or unsafe dialogue) triggered a consistent escalation pipeline and left traceable evidence was critical for system reliability.\n",
    "\n",
    "**Solution:**  \n",
    "Created an `evidence/` directory where both **captured images** and **interaction transcripts** are saved with timestamps.  \n",
    "The escalation module operates in 3 levels:\n",
    "1. **Low alert:** Unrecognized voice or partial face match.\n",
    "2. **Medium alert:** Confirmed mismatch + unexpected dialogue.\n",
    "3. **High alert:** system identifies threat intent → automatic evidence save + optional alert tone.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Audio Engine Compatibility (py-speech →Windows SAPI)\n",
    "Initially, the system used the `pyttsx` or `py-speech` backend for text-to-speech operations. However, these engines had limited compatibility on Windows 10/11, leading to repeated audio initialization failures and inconsistent voice playback.  \n",
    "To address this, the text-to-speech engine was migrated to **`pyttsx3` with the Windows SAPI driver**:\n",
    "\n",
    "\n",
    "\n",
    "##  Ethical Considerations and Testing Results\n",
    "\n",
    "### 1. Privacy and Data Storage\n",
    "The system stores facial embeddings and voice activation data locally within the user’s machine.  \n",
    "**No cloud uploads or external servers** are used — protecting users’ biometric privacy.  \n",
    "\n",
    "\n",
    "**Ethical Safeguards:**\n",
    "- Clear consent: Users explicitly enroll themselves.\n",
    "- Local-only data: No remote APIs are called.\n",
    "- Deletion control: Users can delete their `.npy` files anytime.\n",
    "- Transparency: The system prints clear logs of when data is being captured or used.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Bias and Fairness in Recognition\n",
    "**Observation:**  \n",
    "Like most facial recognition systems, accuracy can vary with lighting, camera angle, and skin tone differences. During testing, recognition under dim lighting showed lower confidence distances (0.48–0.55).\n",
    "\n",
    "**Mitigation Steps:**\n",
    "- Added multiple embeddings per user under varied lighting.\n",
    "- Used Euclidean threshold calibration to balance false acceptance/rejection.\n",
    "- Encouraged inclusion of diverse user samples during enrollment.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Testing Methodology and Results\n",
    "\n",
    "#### Test Setup\n",
    "- **Trusted Faces:** 2 users enrolled (`Rehna`, `Yashaswini`),with 21 embeddings in total.\n",
    "- **Test Cases:** ~60 images under 3 conditions:\n",
    "  - *Bright light*\n",
    "  - *Dim light*\n",
    "  - *Angle variation*\n",
    "- **Threshold:** 0.45 Euclidean distance.\n",
    "\n",
    "#### Results Summary\n",
    "| Metric | Overall | Bright Light | Dim Light | Angled Face |\n",
    "|:--------|:---------:|:------------:|:-----------:|:-------------:|\n",
    "| Accuracy | 0.91 | 0.96 | 0.85 | 0.89 |\n",
    "| Precision | 0.90 | 0.95 | 0.84 | 0.86 |\n",
    "| Recall | 0.88 | 0.94 | 0.82 | 0.85 |\n",
    "| F1 Score | 0.89 | 0.94 | 0.83 | 0.85 |\n",
    "\n",
    "- **Failure cases** mainly occurred when faces were partially occluded or dimly lit.\n",
    "- False positives were rare due to the conservative threshold.\n",
    "- Voice activation worked reliably (>95%) in quiet environments.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Responsible Use and Limitations\n",
    "This system is intended for **personal room security or lab demonstration**, not public surveillance.  \n",
    "Ethical use requires:\n",
    "- Informing anyone whose data is captured.\n",
    "- Avoiding deployment in shared or public spaces.\n",
    "- Ensuring logs are used only for self-monitoring.\n",
    "\n",
    "**Key Takeaway:**  \n",
    "While the system successfully integrates multimodal security under full offline operation, responsible handling of biometric data and explicit user consent remain the most crucial factors in ethical deployment.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d7adb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
